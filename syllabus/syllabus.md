# COMS 4705: Natural Language Processing
**Columbia University - Fall 2024**  
**Professor John Hewitt**

## Course Description

This course provides a comprehensive introduction to Natural Language Processing (NLP), covering both traditional and modern machine learning approaches to understanding and generating human language. Students will learn fundamental algorithms, implement core NLP systems, and explore state-of-the-art neural models including transformers and large language models.

## Learning Outcomes

Upon successful completion of this course, students will be able to:
1. Understand and implement fundamental NLP algorithms (tokenization, parsing, tagging)
2. Design and train neural language models
3. Work with transformer architectures and pre-trained language models
4. Apply NLP techniques to real-world problems
5. Evaluate NLP systems and understand their limitations
6. Consider ethical implications of NLP applications

## Prerequisites

- COMS 3134 (Data Structures in Java)
- COMS 3157 (Advanced Programming)
- Basic knowledge of linear algebra and probability
- Programming experience in Python (recommended)

## Required Materials

### Textbooks
- **Primary**: Jurafsky & Martin, "Speech and Language Processing" (3rd edition, available online)
- **Supplementary**: Eisenstein, "Introduction to Natural Language Processing" (available online)

### Software
- Python 3.8+ with PyTorch, transformers, NLTK, spaCy
- Jupyter Notebooks
- Git and GitHub account

## Course Schedule

### Week 1: Introduction and Foundations
- Course overview and NLP applications
- Text preprocessing and tokenization
- Regular expressions and finite automata

### Week 2: Language Modeling
- N-gram language models
- Smoothing techniques
- Evaluation metrics (perplexity)

### Week 3: Neural Language Models
- Neural network basics for NLP
- Feed-forward neural language models
- Word embeddings (Word2Vec, GloVe)

### Week 4: Sequence Models
- Recurrent Neural Networks (RNNs)
- Long Short-Term Memory (LSTM)
- Bidirectional RNNs

### Week 5: Part-of-Speech Tagging
- HMM taggers
- CRF taggers
- Neural sequence labeling

### Week 6: Parsing
- Context-free grammars
- Chart parsing (CKY algorithm)
- Dependency parsing

### Week 7: Semantics and Word Sense
- Lexical semantics
- Word sense disambiguation
- Semantic role labeling

### Week 8: Machine Translation
- Statistical machine translation
- Neural machine translation
- Attention mechanisms

### Week 9: Transformers and BERT
- Attention is all you need
- BERT and pre-training
- Fine-tuning for downstream tasks

### Week 10: Advanced Transformers
- GPT models and generation
- T5 and text-to-text transfer
- Large language models

### Week 11: Information Extraction
- Named entity recognition
- Relation extraction
- Event extraction

### Week 12: Question Answering
- Reading comprehension
- Open-domain QA
- Knowledge-grounded QA

### Week 13: Dialogue Systems
- Task-oriented dialogue
- Chatbots and conversational AI
- Dialogue state tracking

### Week 14: Ethics and Applications
- Bias in NLP systems
- Privacy and fairness
- Real-world applications

### Week 15: Project Presentations
- Student project presentations
- Course review and wrap-up

## Assignments and Grading

### Grade Breakdown
- **Homework Assignments (40%)**: 4 assignments × 10% each
- **Midterm Exam (20%)**
- **Final Project (30%)**
- **Participation (10%)**: Class participation, quizzes, and discussion forum

### Assignment Schedule
- **Assignment 1**: Language models and text preprocessing (Week 3)
- **Assignment 2**: Neural networks and word embeddings (Week 5)
- **Assignment 3**: Sequence models and POS tagging (Week 7)
- **Assignment 4**: Transformers and fine-tuning (Week 11)

### Final Project
Students will work on a semester-long project involving:
- Literature review and problem formulation
- Implementation of an NLP system
- Experimental evaluation
- Final presentation and report

**Key Dates**:
- Project proposal: Week 6
- Midterm presentation: Week 10
- Final presentation: Week 15
- Final report: Finals week

## Course Policies

### Attendance
Regular attendance is expected. If you must miss class, please notify the instructor in advance and review materials with a classmate.

### Late Policy
- Assignments: 10% penalty per day late
- Project milestones: No extensions except for documented emergencies
- Final project: Late submissions not accepted

### Academic Integrity
All work must be your own. You may discuss concepts with classmates, but all code and written work must be individual effort. Proper citations are required for all sources.

**Collaboration Policy**:
- ✅ Discussing high-level concepts and approaches
- ✅ Helping debug general programming issues
- ❌ Sharing code solutions
- ❌ Copying from online sources without attribution

### Accommodations
Students with documented disabilities should contact the Office of Disability Services to arrange appropriate accommodations.

### Communication
- **Primary**: CourseWorks discussion forum
- **Email**: For private matters only
- **Office Hours**: See schedule on CourseWorks

## Resources

### Online Resources
- [CourseWorks](https://courseworks2.columbia.edu/) - Primary course management
- [Piazza](https://piazza.com/) - Discussion forum
- [GitHub Classroom](https://classroom.github.com/) - Assignment distribution

### Additional Reading
- Papers from ACL, EMNLP, NAACL conferences
- Industry blogs (OpenAI, Google AI, Facebook Research)
- Online courses (CS224N Stanford, CS288 Berkeley)

### Technical Resources
- [Hugging Face](https://huggingface.co/) - Pre-trained models
- [Papers with Code](https://paperswithcode.com/) - Latest research
- [NLP Progress](https://nlpprogress.com/) - Tracking SOTA results

---
*This syllabus is subject to change. Updates will be announced in class and posted on CourseWorks.*