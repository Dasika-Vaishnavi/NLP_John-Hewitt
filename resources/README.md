# Resources - COMS 4705 NLP

This directory contains supplementary resources, reading materials, and reference guides to support your learning in the course.

## üìö Textbooks and Primary Materials

### Required Textbooks
- **Jurafsky & Martin**: [Speech and Language Processing (3rd ed.)](https://web.stanford.edu/~jurafsky/slp3/) - Free online
- **Eisenstein**: [Introduction to Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) - Free PDF

### Supplementary Books
- **Manning & Sch√ºtze**: Foundations of Statistical Natural Language Processing
- **Bird, Klein & Loper**: Natural Language Processing with Python (NLTK Book)
- **Goldberg**: Neural Network Methods for Natural Language Processing
- **Chollet**: Deep Learning with Python

## üîó Online Courses and Tutorials

### Stanford CS224N: Natural Language Processing with Deep Learning
- **Instructor**: Christopher Manning
- **Materials**: [cs224n.stanford.edu](http://cs224n.stanford.edu/)
- **Videos**: Available on YouTube
- **Focus**: Neural approaches to NLP

### Berkeley CS288: Natural Language Processing
- **Instructor**: Dan Klein
- **Materials**: Public course materials available
- **Focus**: Classical and modern NLP methods

### Fast.ai NLP Course
- **Platform**: fast.ai
- **Format**: Practical, code-first approach
- **Focus**: Applying state-of-the-art models

### Hugging Face Course
- **Platform**: [huggingface.co/course](https://huggingface.co/course/)
- **Format**: Interactive online course
- **Focus**: Transformers and modern NLP

## üìñ Essential Papers by Topic

### Foundational Papers
- Shannon (1948): "A Mathematical Theory of Communication"
- Chomsky (1957): "Syntactic Structures"
- Brown et al. (1990): "A Statistical Approach to Machine Translation"

### Language Models
- **N-gram Models**: Kneser & Ney (1995) - "Improved backing-off for M-gram language modeling"
- **Neural LMs**: Bengio et al. (2003) - "A Neural Probabilistic Language Model"
- **Word2Vec**: Mikolov et al. (2013) - "Efficient Estimation of Word Representations"
- **GloVe**: Pennington et al. (2014) - "GloVe: Global Vectors for Word Representation"

### Sequence Models
- **RNNs**: Elman (1990) - "Finding Structure in Time"
- **LSTMs**: Hochreiter & Schmidhuber (1997) - "Long Short-Term Memory"
- **Seq2Seq**: Sutskever et al. (2014) - "Sequence to Sequence Learning"
- **Attention**: Bahdanau et al. (2015) - "Neural Machine Translation by Jointly Learning to Align"

### Transformers and Pre-training
- **Attention**: Vaswani et al. (2017) - "Attention Is All You Need"
- **BERT**: Devlin et al. (2018) - "BERT: Pre-training of Deep Bidirectional Transformers"
- **GPT**: Radford et al. (2018) - "Improving Language Understanding by Generative Pre-Training"
- **T5**: Raffel et al. (2019) - "Exploring the Limits of Transfer Learning"

### Recent Advances
- **GPT-3**: Brown et al. (2020) - "Language Models are Few-Shot Learners"
- **InstructGPT**: Ouyang et al. (2022) - "Training language models to follow instructions"
- **ChatGPT**: OpenAI (2022) - Technical report
- **GPT-4**: OpenAI (2023) - Technical report

## üõ†Ô∏è Technical Resources

### Libraries and Frameworks

#### Core NLP Libraries
- **NLTK**: Natural Language Toolkit for Python
- **spaCy**: Industrial-strength NLP in Python
- **Gensim**: Topic modeling and document similarity
- **TextBlob**: Simple API for common NLP tasks

#### Deep Learning Frameworks
- **PyTorch**: Primary framework for the course
- **TensorFlow**: Alternative deep learning framework
- **JAX**: High-performance ML research
- **Hugging Face Transformers**: Pre-trained transformer models

#### Specialized Tools
- **AllenNLP**: Research library for NLP
- **Flair**: Framework for state-of-the-art NLP
- **scikit-learn**: Machine learning library with text processing
- **Pandas**: Data manipulation and analysis

### Pre-trained Models
- **Hugging Face Model Hub**: [huggingface.co/models](https://huggingface.co/models)
- **OpenAI Models**: GPT family (API access)
- **Google Research**: BERT, T5, PaLM, etc.
- **Facebook Research**: RoBERTa, BART, OPT, etc.

### Datasets
- **Hugging Face Datasets**: [huggingface.co/datasets](https://huggingface.co/datasets)
- **Papers with Code**: [paperswithcode.com/datasets](https://paperswithcode.com/datasets)
- **Google Dataset Search**: [datasetsearch.research.google.com](https://datasetsearch.research.google.com)
- **Kaggle Datasets**: [kaggle.com/datasets](https://kaggle.com/datasets)

## üìä Benchmark Datasets and Evaluations

### General Language Understanding
- **GLUE**: General Language Understanding Evaluation
- **SuperGLUE**: More challenging version of GLUE
- **XTREME**: Cross-lingual benchmark

### Specific Tasks
- **SQuAD**: Reading Comprehension
- **CoNLL**: Named Entity Recognition, POS Tagging
- **WMT**: Machine Translation
- **BLEU/ROUGE**: Automatic evaluation metrics

## üåê Online Communities and Forums

### Research Communities
- **ACL Anthology**: [aclanthology.org](https://aclanthology.org/) - NLP research papers
- **arXiv**: [arxiv.org](https://arxiv.org/list/cs.CL/recent) - Recent preprints
- **Papers with Code**: [paperswithcode.com](https://paperswithcode.com/) - Code for research papers

### Discussion Forums
- **Reddit**: r/MachineLearning, r/LanguageTechnology
- **Stack Overflow**: Programming questions and answers
- **Cross Validated**: Statistics and ML Q&A
- **Hugging Face Community**: Model and dataset discussions

### Conferences and Workshops
- **ACL**: Association for Computational Linguistics (main conference)
- **EMNLP**: Conference on Empirical Methods in NLP
- **NAACL**: North American Chapter of ACL
- **COLING**: International Conference on Computational Linguistics

## üîß Development Tools

### IDEs and Editors
- **Jupyter Notebooks**: Interactive development
- **VS Code**: Popular code editor with Python support
- **PyCharm**: Full-featured Python IDE
- **Google Colab**: Cloud-based Jupyter environment

### Version Control
- **Git**: Distributed version control
- **GitHub**: Code hosting and collaboration
- **GitLab**: Alternative to GitHub
- **DVC**: Data Version Control for ML

### Experiment Tracking
- **Weights & Biases**: Experiment tracking and visualization
- **TensorBoard**: TensorFlow's visualization toolkit
- **MLflow**: Open-source ML lifecycle management
- **Neptune**: Experiment management platform

## üìù Writing and Presentation

### Academic Writing
- **LaTeX**: Document preparation system
- **Overleaf**: Online LaTeX editor
- **Zotero**: Reference management
- **Mendeley**: Alternative reference manager

### Presentation Tools
- **Beamer**: LaTeX presentation class
- **Reveal.js**: HTML presentations
- **PowerPoint/Keynote**: Traditional presentation software

## üéØ Practice and Challenges

### Coding Challenges
- **Kaggle Competitions**: Real-world ML challenges
- **HackerRank**: Programming challenges including NLP
- **LeetCode**: Algorithm and data structure problems
- **Project Euler**: Mathematical/computational problems

### NLP-Specific Challenges
- **SemEval**: Semantic Evaluation workshops
- **WMT Shared Tasks**: Machine Translation challenges
- **CONLL Shared Tasks**: Various NLP tasks

## üìà Staying Current

### Blogs and News
- **The Gradient**: AI research blog
- **Towards Data Science**: Medium publication
- **Google AI Blog**: Google's AI research updates
- **OpenAI Blog**: OpenAI's research and product updates

### Newsletters
- **The Batch**: Andrew Ng's AI newsletter
- **AI Research**: Weekly AI research updates
- **NLP News**: Curated NLP content

### Podcasts
- **TWIML AI**: This Week in Machine Learning & AI
- **The NVIDIA AI Podcast**: AI interviews and discussions
- **Practical AI**: Practical applications of AI

## üí° Tips for Using Resources

### Effective Reading
1. Start with survey papers for new topics
2. Read abstracts and conclusions first
3. Focus on methodology and experimental setup
4. Take notes and summarize key insights

### Code Exploration
1. Start with official tutorials and documentation
2. Explore example code before writing your own
3. Use pre-trained models as starting points
4. Contribute to open-source projects when possible

### Staying Organized
1. Use reference managers for papers
2. Bookmark useful resources
3. Keep a learning journal
4. Share resources with classmates

---

**Need help finding specific resources?** Ask on the course forum or during office hours!